{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ae7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "for j in range(2020, 2023):\n",
    "    for i in range(12):\n",
    "        files.append(\"traffic_density_{}\".format(str(j) + str(i+1).zfill(2)))\n",
    "        \n",
    "\n",
    "for i in range(4):\n",
    "    files.append(\"traffic_density_{}\".format(str(2023) + str(i+1).zfill(2)))\n",
    "        \n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a09b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_conservation = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    \n",
    "    node_loc = {}\n",
    "        \n",
    "    traffic_data = pd.read_csv(file + \".csv\", encoding = \"utf-8\")\n",
    "    G = nx.read_gexf(file + \".gexf\")\n",
    "\n",
    "    actual = G.number_of_nodes()\n",
    "    largest_component = max(nx.connected_components(G), key=len)\n",
    "    temp = G.subgraph(largest_component)\n",
    "    G_l = nx.Graph(temp)\n",
    "    \n",
    "    traffic_data = traffic_data[[\"GEOHASH\", \"LATITUDE\", \"LONGITUDE\"]].set_index(\"GEOHASH\")\n",
    "    \n",
    "    for row in traffic_data.iterrows():\n",
    "        if row[0] not in node_loc:\n",
    "            node_loc[row[0]] = {\"LAT\": row[1][\"LONGITUDE\"], \"LONG\": row[1][\"LATITUDE\"]}\n",
    "\n",
    "    nx.set_node_attributes(G_l, node_loc)\n",
    "    largest = G_l.number_of_nodes()\n",
    "    \n",
    "    node_conservation.append(largest/actual)\n",
    "    \n",
    "    nx.write_gexf(G_l, file + \"_largest.gexf\")\n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for edge in list(G.edges(data = True)):\n",
    "        mult = 1\n",
    "        if edge[2][\"isNegative\"]:\n",
    "            mult = -1\n",
    "        data.append([\n",
    "            edge[0], edge[1], edge[2][\"dist\"], edge[2][\"weight\"] * mult\n",
    "        ])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns = [\"GEOHASH_1\", \"GEOHASH_2\", \"Distance\", \"Correlation\"])\n",
    "    df.to_csv(file + \"_correlations.csv\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ef653",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae37fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(node_conservation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 0.21540647846367683\n",
    "min_val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_nodes = {}\n",
    "scaler = MinMaxScaler(feature_range = (min_val, max_val))\n",
    "\n",
    "for file in tqdm(files):\n",
    "    G = nx.read_gexf(file + \".gexf\")\n",
    "    btw_nodes = nx.betweenness_centrality(G)\n",
    "    btw_centr = np.array(list(btw_nodes.values()))\n",
    "    keys = list(btw_nodes.keys())\n",
    "    btw_centr_scaled = scaler.fit_transform(btw_centr.reshape(-1, 1))\n",
    "    btw_centr_scaled_list = [btw_centr_scaled[i][0] for i in range(len(btw_centr_scaled))]\n",
    "    for j in range(len(keys)):\n",
    "        if keys[j] not in between_nodes:\n",
    "            between_nodes[keys[j]] = [btw_centr_scaled[j], 1]\n",
    "        else:\n",
    "            prev_val = between_nodes[keys[j]][0]\n",
    "            prev_count = between_nodes[keys[j]][1]\n",
    "            new_count = prev_count + 1\n",
    "            new_val = (btw_centr_scaled[j] + prev_val * prev_count) / (new_count)\n",
    "            between_nodes[keys[j]] = [new_val, new_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102736c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "between_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f560b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btw_centr = {loc: [between_nodes[loc][0][0], between_nodes[loc][1]] for loc in list(between_nodes.keys())}\n",
    "\n",
    "btw_centr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a87c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(btw_centr.keys())\n",
    "vals = list(btw_centr.values())\n",
    "vals_real = [vals[i][0] for i in range(len(vals)) if vals[i][1] >= 24]\n",
    "top_5 = sorted(vals_real, reverse = True)[:5]\n",
    "\n",
    "top_5_keys = []\n",
    "\n",
    "for key in keys:\n",
    "    for val in top_5:\n",
    "        if val == btw_centr[key][0]:\n",
    "            top_5_keys.append(key)\n",
    "        \n",
    "top_5_dict = {key: btw_centr[key] for key in top_5_keys}\n",
    "\n",
    "btw_centralities = [btw_centr[key][0] for key in top_5_keys]\n",
    "\n",
    "counts = [btw_centr[key][1] for key in top_5_keys]\n",
    "\n",
    "file = \"traffic_density_202005.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "org_df = df[df[\"GEOHASH\"].isin(top_5_keys)]\n",
    "\n",
    "new_df = org_df.groupby(\"GEOHASH\").agg({\"LATITUDE\": np.mean, \"LONGITUDE\": np.mean}).reset_index()\n",
    "\n",
    "new_df.columns = [\"GEOHASH\", \"LONGITUDE\", \"LATITUDE\"]\n",
    "\n",
    "new_df[\"CENTRALITY\"] = btw_centralities\n",
    "\n",
    "new_df = new_df.sort_values(\"CENTRALITY\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "my_map = folium.Map()\n",
    "\n",
    "# Add markers for each coordinate\n",
    "for _, row in new_df.iterrows():\n",
    "    folium.Marker([row[\"LATITUDE\"], row[\"LONGITUDE\"]]).add_to(my_map)\n",
    "    \n",
    "my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2802233",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9df03",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_map = folium.Map()\n",
    "\n",
    "# Add markers for each coordinate\n",
    "for _, row in new_df.iterrows():\n",
    "    folium.Marker([row[\"LATITUDE\"], row[\"LONGITUDE\"]]).add_to(my_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_hash = []\n",
    "maxes = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    G = nx.read_gexf(file + \".gexf\")\n",
    "    btw_nodes = nx.betweenness_centrality(G)\n",
    "    btw_centr = np.array(list(btw_nodes.values()))\n",
    "    max_val = max(btw_centr)\n",
    "    maxes.append(max_val)\n",
    "    keys = list(btw_nodes.keys())\n",
    "    for key in keys:\n",
    "        if btw_nodes[key] == max_val:\n",
    "            max_hash.append(key)\n",
    "            \n",
    "max_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd284da",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"traffic_density_202005.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "org_df = df[df[\"GEOHASH\"].isin(max_hash)]\n",
    "\n",
    "new_df = org_df.groupby(\"GEOHASH\").agg({\"LATITUDE\": np.mean, \"LONGITUDE\": np.mean}).reset_index()\n",
    "\n",
    "new_df.columns = [\"GEOHASH\", \"LONGITUDE\", \"LATITUDE\"]\n",
    "\n",
    "counts = []\n",
    "\n",
    "for idx, row in new_df.iterrows():\n",
    "    counts.append(max_hash.count(row[\"GEOHASH\"]))\n",
    "    \n",
    "new_df[\"BTW_COUNTS\"] = counts\n",
    "\n",
    "new_df\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for _, row in new_df.iterrows():\n",
    "    G.add_node(row[\"GEOHASH\"], lat = row[\"LATITUDE\"], long = row[\"LONGITUDE\"], count = row[\"BTW_COUNTS\"])\n",
    "    \n",
    "    \n",
    "for _, row in df.iterrows():\n",
    "    if max_hashes.index(row[\"GEOHASH\"]) == -1:\n",
    "        pass\n",
    "    \n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e542e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {loc: between_nodes[loc][0][0] for loc in list(between_nodes.keys())}\n",
    "values = list(new_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(between_nodes.values())\n",
    "values = [vals[i][0][0] for i in range(len(vals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = values.index(max(values))\n",
    "list(between_nodes.keys())[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map object\n",
    "my_map = folium.Map()\n",
    "\n",
    "# Add markers for each coordinate\n",
    "for _, row in new_df.iterrows():\n",
    "    folium.Marker([row[\"LATITUDE\"], row[\"LONGITUDE\"]]).add_to(my_map)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "my_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
